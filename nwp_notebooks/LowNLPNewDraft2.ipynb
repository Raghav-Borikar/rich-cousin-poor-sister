{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75cfd74c-1d6d-4525-a802-d971d77d1579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\pc\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\pc\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.19.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a40d27-55eb-46ba-bc65-f4f15b6c03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c2a884-fef8-4bba-93ac-0729ed97c57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\pc\\anaconda3\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.12 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\pc\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\pc\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\pc\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\pc\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef90a5ba-a260-4b4e-b1b2-0a8367e2a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\pc\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install sentencepiece transformers --upgrade\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741df775-a547-4401-a74f-d841f7729944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\pc\\anaconda3\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.6.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19788ec5-b1ff-48ea-8d76-4e884f1e022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize environment and imports correctly\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Enable synchronous CUDA errors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DistilBertForMaskedLM, AlbertTokenizer, AutoConfig\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from contextlib import nullcontext\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import GradScaler\n",
    "import bitsandbytes as bnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066b0167-49ff-4396-a626-fe21b9e83401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b90a42e-01c9-4a16-9777-97f6642154b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 219: Config (Add distillation hyperparameters)\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Model parameters\n",
    "        self.teacher_model_name = \"ai4bharat/indic-bert\"\n",
    "        self.student_model_name = \"distilbert-base-multilingual-cased\"\n",
    "        self.max_length = 64\n",
    "        self.batch_size = 32 # Reduced from 32 in logs, check GPU memory\n",
    "        self.learning_rate = 5e-5 # Teacher LR\n",
    "        self.weight_decay = 0.01\n",
    "        self.teacher_epochs = 1\n",
    "        self.distillation_epochs = 1 # Student epochs\n",
    "        self.warmup_steps = 500 # Reduced warmup\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Language parameters\n",
    "        self.teacher_lang = \"hi\" # Hindi\n",
    "        self.student_lang = \"hne\" # Chhattisgarhi\n",
    "\n",
    "        # --- Distillation / RL parameters ---\n",
    "        self.rl_lr = 3e-5 # Student LR (Adjusted)\n",
    "        # Knowledge Distillation (KD) specific parameters\n",
    "        self.distillation_temp = 2.0 # Temperature for softening probabilities\n",
    "        self.distill_loss_weight = 0.5 # Weight for KLDiv loss vs CE loss (0.0 means only CE, 1.0 means only KD)\n",
    "        self.ce_loss_weight = 1.0 - self.distill_loss_weight # Weight for CrossEntropy loss with hard labels\n",
    "\n",
    "        # (Optional RL params - might not be needed if focusing on KD)\n",
    "        self.gamma = 0.99 # Discount factor (only if using RL rewards)\n",
    "        self.entropy_coef = 0.01 # Entropy coefficient (only if using RL action sampling loss)\n",
    "\n",
    "        self.few_shot_examples = 5 # Kept from original\n",
    "\n",
    "        # Dataset\n",
    "        self.train_test_split = 0.1\n",
    "\n",
    "        # Paths\n",
    "        self.output_dir = \"output/\"\n",
    "        self.log_dir = \"logs/\"\n",
    "        self.model_dir = \"models/\"\n",
    "\n",
    "        # Evaluation & Checkpointing\n",
    "        self.eval_every = 200 # Evaluate every N **update steps** (not batches)\n",
    "        self.save_every = 500 # Save model every N **update steps**\n",
    "        self.early_stopping_patience = 3 # Based on validation accuracy\n",
    "\n",
    "        # --- Numerical Stability ---\n",
    "        self.logit_clamp_value = 30.0 # Clamp logits to prevent extreme values before softmax/log_softmax\n",
    "        self.label_smoothing = 0.1 # Label smoothing for CrossEntropyLoss\n",
    "\n",
    "        # Create directories\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d13b75-fbc1-48d8-a1eb-36f305c92b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, model_name, tokenizer_vocab_size=None):\n",
    "        super().__init__()\n",
    "        # Load config first\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        original_vocab_size = self.config.vocab_size\n",
    "        logger.info(f\"Original model vocab size: {original_vocab_size}\")\n",
    "        \n",
    "        # Initialize model on CPU with float32\n",
    "        self.bert_model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name,\n",
    "            config=self.config,\n",
    "            torch_dtype=torch.float32  # Force FP32 initialization\n",
    "        )\n",
    "        \n",
    "        # Resize embeddings if needed BEFORE moving to GPU\n",
    "        if tokenizer_vocab_size and tokenizer_vocab_size != self.config.vocab_size:\n",
    "            logger.info(f\"Resizing embeddings from {self.config.vocab_size} to {tokenizer_vocab_size}\")\n",
    "            self._safe_resize_embeddings(tokenizer_vocab_size)\n",
    "            \n",
    "            # Verify resize was successful\n",
    "            if self.bert_model.config.vocab_size != tokenizer_vocab_size:\n",
    "                logger.error(f\"Resize failed! Current size: {self.bert_model.config.vocab_size}\")\n",
    "                raise ValueError(\"Embedding resize operation failed\")\n",
    "        \n",
    "        # Initialize adapter layer with verified vocab size\n",
    "        self.hidden_size = self.config.hidden_size\n",
    "        self.vocab_size = self.bert_model.config.vocab_size\n",
    "        self.next_token_adapter = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        logger.info(f\"TeacherModel initialized with vocab_size={self.vocab_size}\")\n",
    "    \n",
    "    def _safe_resize_embeddings(self, new_vocab_size):\n",
    "        \"\"\"Improved embedding resize with better validation\"\"\"\n",
    "        old_embeddings = self.bert_model.get_input_embeddings()\n",
    "        old_size = old_embeddings.num_embeddings\n",
    "        \n",
    "        # Skip if new size is smaller (would truncate vocabulary)\n",
    "        if new_vocab_size <= old_size:\n",
    "            logger.warning(f\"New vocab size {new_vocab_size} <= current size {old_size}. Skipping resize.\")\n",
    "            return\n",
    "        \n",
    "        # Perform resize\n",
    "        new_embeddings = self.bert_model.resize_token_embeddings(new_vocab_size)\n",
    "        \n",
    "        # Initialize new embedding weights with normal distribution\n",
    "        if new_vocab_size > old_size:\n",
    "            with torch.no_grad():\n",
    "                # Calculate statistics of existing embeddings for initialization\n",
    "                mean = old_embeddings.weight.data.mean().item()\n",
    "                std = old_embeddings.weight.data.std().item()\n",
    "                \n",
    "                # Initialize new tokens with appropriate distribution\n",
    "                new_embeddings.weight.data[old_size:] = torch.normal(\n",
    "                    mean=mean, std=std,\n",
    "                    size=(new_vocab_size - old_size, self.config.hidden_size)\n",
    "                )\n",
    "        \n",
    "        # Update all necessary config values\n",
    "        self.bert_model.config.vocab_size = new_vocab_size\n",
    "        self.config.vocab_size = new_vocab_size\n",
    "        logger.info(f\"Embeddings successfully resized to {new_vocab_size}\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Validate input tensor indices\n",
    "        if torch.min(input_ids) < 0 or torch.max(input_ids) >= self.vocab_size:\n",
    "            min_id = torch.min(input_ids).item()\n",
    "            max_id = torch.max(input_ids).item()\n",
    "            logger.error(f\"Invalid input_ids detected! Range: [{min_id}, {max_id}], Vocab size: {self.vocab_size}\")\n",
    "            raise ValueError(f\"Input IDs must be within range [0, {self.vocab_size-1}]\")\n",
    "        \n",
    "        # Create token_type_ids tensor (required for Albert models)\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        token_type_ids = torch.zeros(batch_size, seq_length, \n",
    "                                   dtype=torch.long, device=input_ids.device)\n",
    "        \n",
    "        # Use try/except to catch any forward pass errors\n",
    "        try:\n",
    "            with torch.amp.autocast(device_type=input_ids.device.type):\n",
    "                outputs = self.bert_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                \n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                return self.next_token_adapter(last_hidden_state)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in forward pass: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7986f9c-938e-494a-bf69-aed36c942be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(StudentModel, self).__init__()\n",
    "        # Use DistilBERT as the base model\n",
    "        self.model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Add a next token prediction head\n",
    "        self.vocab_size = self.model.config.vocab_size\n",
    "        self.hidden_size = self.model.config.dim\n",
    "        self.next_token_head = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        logits = self.next_token_head(hidden_states)\n",
    "        return logits\n",
    "    \n",
    "    # Method for RL action and value calculation\n",
    "    def get_action_and_value(self, input_ids, attention_mask=None):\n",
    "        # Get logits for the entire sequence\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        \n",
    "        # Extract only the last token position logits for next token prediction\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()  # Shape: [batch_size]\n",
    "        \n",
    "        # Get log probability of the action\n",
    "        log_prob = dist.log_prob(action)  # Shape: [batch_size]\n",
    "        \n",
    "        # Calculate entropy for exploration encouragement\n",
    "        entropy = dist.entropy().mean()  # Scalar\n",
    "        \n",
    "        return action, log_prob, entropy, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e458d1-7046-4b51-b8fc-eeed7e09f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: DataProcessor (Refined Pad Token and Vocab Size Logging)\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        # Load tokenizer first\n",
    "        self._load_tokenizers()\n",
    "        \n",
    "        # Verify embedding compatibility\n",
    "        self._verify_embedding_sizes()\n",
    "\n",
    "    def _load_tokenizers(self):\n",
    "        # Teacher tokenizer with AlbertTokenizer\n",
    "        self.teacher_tokenizer = AlbertTokenizer.from_pretrained(\n",
    "            self.config.teacher_model_name,\n",
    "            keep_accents=True\n",
    "        )\n",
    "        \n",
    "        # Add [PAD] token if missing\n",
    "        if self.teacher_tokenizer.pad_token is None:\n",
    "            self.teacher_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            \n",
    "        self.teacher_pad_id = self.teacher_tokenizer.pad_token_id\n",
    "        self.teacher_vocab_size = len(self.teacher_tokenizer)\n",
    "\n",
    "        # Student tokenizer (DistilBERT)\n",
    "        self.student_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.student_model_name\n",
    "        )\n",
    "\n",
    "    def _verify_embedding_sizes(self):\n",
    "        \"\"\"Ensure tokenizer and model vocab sizes match before model creation\"\"\"\n",
    "        teacher_config = AutoConfig.from_pretrained(self.config.teacher_model_name)\n",
    "        if teacher_config.vocab_size != self.teacher_vocab_size:\n",
    "            logger.warning(f\"Tokenizer vocab ({self.teacher_vocab_size}) ≠ model vocab ({teacher_config.vocab_size})\")\n",
    "    def load_dataset(self):\n",
    "        # ... (Keep trust_remote_code=True) ...\n",
    "        try:\n",
    "            logger.info(\"Loading NLLB dataset for Hindi-Chhattisgarhi pair\")\n",
    "            nllb_dataset = load_dataset(\"allenai/nllb\", \"hin_Deva-hne_Deva\", trust_remote_code=True)\n",
    "            hindi_samples = [{\"lang\": self.config.teacher_lang, \"text\": item['translation']['hin_Deva']} for item in nllb_dataset[\"train\"]]\n",
    "            chhattisgarhi_samples = [{\"lang\": self.config.student_lang, \"text\": item['translation']['hne_Deva']} for item in nllb_dataset[\"train\"]]\n",
    "            logger.info(f\"Loaded {len(hindi_samples)} Hindi, {len(chhattisgarhi_samples)} Chhattisgarhi samples\")\n",
    "            return hindi_samples, chhattisgarhi_samples\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading dataset: {e}\", exc_info=True) # Log traceback\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32461bc-a6a0-4195-bfe6-d6ab18823209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume this function is defined in a previous cell (e.g., Cell 225) or at the top of Cell 233\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Make sure tqdm is imported\n",
    "\n",
    "def validate_dataset(dataset, tokenizer, name=\"dataset\", sample_size=None):\n",
    "    \"\"\"Ensure all token IDs in the dataset are valid against the tokenizer's vocab size.\"\"\"\n",
    "    vocab_size = len(tokenizer)\n",
    "    logger.info(f\"Validating {name} (Tokenizer Vocab Size: {vocab_size})...\")\n",
    "\n",
    "    num_samples_to_check = len(dataset) if sample_size is None else min(sample_size, len(dataset))\n",
    "    indices_to_check = range(num_samples_to_check) if sample_size is None else np.random.choice(len(dataset), num_samples_to_check, replace=False)\n",
    "    logger.info(f\"Checking {num_samples_to_check} samples.\")\n",
    "\n",
    "    invalid_count = 0\n",
    "    first_invalid_idx = -1\n",
    "    first_invalid_ids = None\n",
    "\n",
    "    for i in tqdm(indices_to_check, desc=f\"Validating {name}\"):\n",
    "        try:\n",
    "            sample = dataset[i]\n",
    "            # Ensure 'input_ids' exists and is a tensor\n",
    "            if 'input_ids' not in sample or not isinstance(sample['input_ids'], torch.Tensor):\n",
    "                 logger.warning(f\"Sample {i} in {name} is missing 'input_ids' tensor. Skipping.\")\n",
    "                 invalid_count +=1 # Count as invalid if structure is wrong\n",
    "                 if first_invalid_idx == -1: first_invalid_idx = i\n",
    "                 continue\n",
    "\n",
    "            input_ids = sample['input_ids']\n",
    "\n",
    "            # Check for invalid IDs (negative or >= vocab_size)\n",
    "            min_id_val = torch.min(input_ids).item()\n",
    "            max_id_val = torch.max(input_ids).item()\n",
    "\n",
    "            if min_id_val < 0 or max_id_val >= vocab_size:\n",
    "                invalid_count += 1\n",
    "                if first_invalid_idx == -1: # Log first error in detail\n",
    "                    first_invalid_idx = i\n",
    "                    first_invalid_ids = input_ids.tolist() # Get the problematic IDs\n",
    "                    logger.error(f\"!!! Invalid token ID found in {name} at index {i} !!!\")\n",
    "                    logger.error(f\"    Range Found: [{min_id_val}, {max_id_val}], Required Range: [0, {vocab_size-1}]\")\n",
    "                    logger.error(f\"    Problematic IDs (sample): {first_invalid_ids[:20]}...\") # Show beginning of IDs\n",
    "                    # Try decoding for context\n",
    "                    try:\n",
    "                        # Filter out only potentially valid IDs for decoding attempt\n",
    "                        valid_range_ids = [id_val for id_val in first_invalid_ids if 0 <= id_val < vocab_size]\n",
    "                        if valid_range_ids:\n",
    "                             decoded = tokenizer.decode(valid_range_ids, skip_special_tokens=False)\n",
    "                             logger.error(f\"    Partial Decode Attempt of valid-range IDs: '{decoded}'\")\n",
    "                        else:\n",
    "                             logger.error(f\"    Cannot decode, no IDs were within the valid range [0, {vocab_size-1}]\")\n",
    "                    except Exception as decode_e:\n",
    "                        logger.error(f\"    Could not decode IDs: {decode_e}\")\n",
    "\n",
    "        except IndexError:\n",
    "            logger.error(f\"IndexError accessing sample {i} in {name}. Dataset length reported as {len(dataset)}.\")\n",
    "            invalid_count += 1\n",
    "            if first_invalid_idx == -1: first_invalid_idx = i\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing item at index {i} in {name}: {e}\", exc_info=True)\n",
    "            invalid_count += 1 # Count other errors as invalid\n",
    "            if first_invalid_idx == -1: first_invalid_idx = i\n",
    "\n",
    "    if invalid_count > 0:\n",
    "        logger.error(f\"Validation FAILED for {name}. Found {invalid_count} invalid samples out of {num_samples_to_check} checked.\")\n",
    "        logger.error(f\"First invalid sample detected at index: {first_invalid_idx}\")\n",
    "        return False\n",
    "    else:\n",
    "        logger.info(f\"Dataset {name} validation passed ({num_samples_to_check} samples checked).\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "742f527c-7e25-4544-ab37-2746a8248cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: NextWordPredictionDataset (Add check for pad token ID)\n",
    "class NextWordPredictionDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.vocab_size = len(tokenizer) # Store vocab size\n",
    "\n",
    "        if self.pad_token_id is None:\n",
    "             logger.error(f\"CRITICAL: Tokenizer {tokenizer.name_or_path} provided to dataset without a valid pad_token_id!\")\n",
    "             # Attempt recovery or raise error earlier? For now, use default 0.\n",
    "             self.pad_token_id = 0\n",
    "\n",
    "        self.examples = [s[\"text\"] for s in samples if isinstance(s.get(\"text\"), str) and len(s[\"text\"]) > 10]\n",
    "        if len(self.examples) < len(samples):\n",
    "             logger.warning(f\"Filtered out {len(samples) - len(self.examples)} samples due to length or type.\")\n",
    "        logger.info(f\"Created dataset with {len(self.examples)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of bounds\")\n",
    "\n",
    "        text = self.examples[idx]\n",
    "        try:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"].squeeze(0) # Remove batch dim\n",
    "            attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "            # Check for invalid IDs immediately after tokenization\n",
    "            if torch.any(input_ids >= self.vocab_size) or torch.any(input_ids < 0):\n",
    "                 logger.error(f\"Invalid IDs found *during tokenization* for index {idx}, text: '{text[:100]}...'\")\n",
    "                 logger.error(f\"Min: {torch.min(input_ids)}, Max: {torch.max(input_ids)}, Vocab Size: {self.vocab_size}\")\n",
    "                 # Replace invalid IDs with UNK token ID? Or skip sample? Skipping is safer.\n",
    "                 # For now, let's just log and proceed, the check in train_teacher will catch it.\n",
    "                 # Or raise an error here:\n",
    "                 # raise ValueError(\"Invalid token IDs generated by tokenizer.\")\n",
    "\n",
    "            # Handle empty/short sequences after tokenization/padding\n",
    "            if input_ids.numel() <= 1:\n",
    "                bos = self.tokenizer.cls_token_id if self.tokenizer.cls_token_id is not None else self.tokenizer.bos_token_id\n",
    "                eos = self.tokenizer.sep_token_id if self.tokenizer.sep_token_id is not None else self.tokenizer.eos_token_id\n",
    "                bos = bos if bos is not None else self.pad_token_id\n",
    "                eos = eos if eos is not None else self.pad_token_id\n",
    "                input_ids = torch.tensor([bos] + [self.pad_token_id] * (self.max_length - 1), dtype=torch.long)\n",
    "                attention_mask = torch.tensor([1] + [0] * (self.max_length - 1), dtype=torch.long)\n",
    "                logger.warning(f\"Handling short/empty sequence for index {idx}\")\n",
    "\n",
    "            # Create labels (shifted input_ids)\n",
    "            labels = input_ids.clone()\n",
    "            # Shift - use pad_token_id for positions where we don't predict\n",
    "            labels[:-1] = input_ids[1:]\n",
    "            labels[-1] = self.pad_token_id # No label for the last token prediction\n",
    "\n",
    "            # Input should be sequence up to second-to-last token\n",
    "            model_input_ids = input_ids[:-1]\n",
    "            model_attention_mask = attention_mask[:-1]\n",
    "            model_labels = labels[:-1] # Labels correspond to model_input_ids positions\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": model_input_ids,\n",
    "                \"attention_mask\": model_attention_mask,\n",
    "                \"labels\": model_labels\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing item at index {idx}, text: '{text[:100]}...': {e}\", exc_info=True)\n",
    "            # Return a dummy item or skip? Returning dummy might hide errors.\n",
    "            # Let's create a fully padded dummy item.\n",
    "            pad_id = self.pad_token_id\n",
    "            dummy_input_ids = torch.full((self.max_length - 1,), pad_id, dtype=torch.long)\n",
    "            dummy_attn_mask = torch.zeros((self.max_length - 1,), dtype=torch.long)\n",
    "            dummy_labels = torch.full((self.max_length - 1,), pad_id, dtype=torch.long)\n",
    "            return {\"input_ids\": dummy_input_ids, \"attention_mask\": dummy_attn_mask, \"labels\": dummy_labels}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc76f74a-2c65-4c88-aa56-ca30dd63bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, few_shot_examples=5, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.few_shot_examples = few_shot_examples\n",
    "        \n",
    "        # Ensure pad token is set\n",
    "        if self.tokenizer.pad_token is None and hasattr(self.tokenizer, 'eos_token'):\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Process samples\n",
    "        self.examples = []\n",
    "        self.few_shot_pool = []\n",
    "        \n",
    "        # Select few-shot examples\n",
    "        valid_samples = [s for s in samples if len(s[\"text\"]) > 10]\n",
    "        if len(valid_samples) > few_shot_examples:\n",
    "            self.few_shot_pool = valid_samples[:few_shot_examples]\n",
    "            self.examples = valid_samples[few_shot_examples:]\n",
    "        else:\n",
    "            self.examples = valid_samples\n",
    "            \n",
    "        logger.info(f\"Created few-shot dataset with {len(self.examples)} examples and {len(self.few_shot_pool)} few-shot examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for dataset with {len(self.examples)} examples\")\n",
    "        \n",
    "        # Get the target example\n",
    "        target_text = self.examples[idx][\"text\"]\n",
    "        \n",
    "        # Build few-shot context with examples and their completions\n",
    "        few_shot_context = \"\"\n",
    "        for example in self.few_shot_pool:\n",
    "            # Split text to simulate next word prediction\n",
    "            words = example[\"text\"].split()\n",
    "            if len(words) > 1:\n",
    "                prefix = \" \".join(words[:-1])\n",
    "                target = words[-1]\n",
    "                few_shot_context += f\"Text: {prefix}\\nNext word: {target}\\n\\n\"\n",
    "        \n",
    "        # Add the target without completion\n",
    "        target_words = target_text.split()\n",
    "        if len(target_words) > 1:\n",
    "            prefix = \" \".join(target_words[:-1])\n",
    "            target = target_words[-1]\n",
    "            few_shot_context += f\"Text: {prefix}\\nNext word:\"\n",
    "            \n",
    "            # Tokenize few-shot context\n",
    "            context_encoding = self.tokenizer(few_shot_context, max_length=self.max_length, \n",
    "                                            padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            # Tokenize target (the expected next word)\n",
    "            target_encoding = self.tokenizer(target, return_tensors=\"pt\")\n",
    "            \n",
    "            # Ensure we have valid tensors\n",
    "            input_ids = context_encoding[\"input_ids\"].squeeze()\n",
    "            attention_mask = context_encoding[\"attention_mask\"].squeeze()\n",
    "            target_id = target_encoding[\"input_ids\"].squeeze()[0]  # Just take the first token\n",
    "            \n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": target_id\n",
    "            }\n",
    "        else:\n",
    "            # Handle edge case of single word\n",
    "            return self.__getitem__((idx + 1) % len(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b617dd0-c672-4132-a869-67d1e4c7bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(teacher_probs, student_action, target_tokens, alpha=0.7):\n",
    "    \"\"\"Improved reward function with better learning signals\"\"\"\n",
    "    batch_size = teacher_probs.shape[0]\n",
    "    reward = torch.zeros(batch_size, device=teacher_probs.device)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Higher reward for matching the target\n",
    "        if student_action[i].item() == target_tokens[i].item():\n",
    "            reward[i] += 5.0\n",
    "        else:\n",
    "            # Small penalty for wrong answers to speed up learning\n",
    "            reward[i] -= 0.1\n",
    "        \n",
    "        # Add partial reward for being close (teacher had high probability for the correct token)\n",
    "        if target_tokens[i].item() < teacher_probs.shape[1]:\n",
    "            teacher_confidence_for_correct = teacher_probs[i, target_tokens[i].item()]\n",
    "            reward[i] += alpha * teacher_confidence_for_correct\n",
    "        \n",
    "        # Add smaller reward based on teacher probability of student's action\n",
    "        token_idx = student_action[i].item()\n",
    "        if token_idx < teacher_probs.shape[1]:\n",
    "            teacher_confidence = teacher_probs[i, token_idx]\n",
    "            reward[i] += (alpha * 0.5) * teacher_confidence\n",
    "    \n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2716e3-4fc7-4942-8fcf-24021d93e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:27:34,102 - __main__ - INFO - bitsandbytes found, will use AdamW8bit.\n"
     ]
    }
   ],
   "source": [
    "# Cell 228: train_teacher (Fix gradient checkpointing call, add 8-bit AdamW, fix GradScaler)\n",
    "\n",
    "# Ensure necessary imports are available\n",
    "from torch.cuda.amp import GradScaler # Keep old import if using older PyTorch, prefer torch.amp.cuda otherwise\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW # Keep standard AdamW import for fallback if needed\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Attempt to import bitsandbytes for 8-bit AdamW\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    bnb_available = True\n",
    "    logger.info(\"bitsandbytes found, will use AdamW8bit.\")\n",
    "except ImportError:\n",
    "    logger.warning(\"bitsandbytes not found. Falling back to standard AdamW. \"\n",
    "                   \"Install with 'pip install bitsandbytes' for memory savings.\")\n",
    "    bnb_available = False\n",
    "\n",
    "logger = logging.getLogger(__name__) # Ensure logger is defined\n",
    "\n",
    "\n",
    "def train_teacher(model, train_loader, val_loader, config, data_processor): # Added data_processor\n",
    "    logger.info(\"Starting teacher model fine-tuning for Hindi...\")\n",
    "    model.to(config.device)\n",
    "\n",
    "    # --- FIX: Call gradient checkpointing on the underlying bert_model ---\n",
    "    if hasattr(model, 'bert_model') and config.device.type == 'cuda':\n",
    "         try:\n",
    "             model.bert_model.gradient_checkpointing_enable()\n",
    "             logger.info(\"Gradient Checkpointing Enabled on model.bert_model.\")\n",
    "         except AttributeError:\n",
    "              logger.warning(\"model.bert_model does not have gradient_checkpointing_enable. Skipping.\")\n",
    "         except Exception as e:\n",
    "              logger.error(f\"Error enabling gradient checkpointing: {e}. Skipping.\")\n",
    "    elif config.device.type == 'cuda':\n",
    "         logger.warning(\"Gradient checkpointing requested but model object doesn't have 'bert_model' attribute.\")\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # --- Use 8-bit AdamW if available, otherwise standard AdamW ---\n",
    "    if bnb_available:\n",
    "        optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    else:\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # Adjust total_steps for gradient accumulation\n",
    "    num_update_steps_per_epoch = len(train_loader) // config.gradient_accumulation_steps if len(train_loader) > 0 else 0\n",
    "    total_steps = num_update_steps_per_epoch * config.teacher_epochs\n",
    "    num_warmup_steps = min(config.warmup_steps, total_steps // 10) if total_steps > 0 else 0\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=max(1, total_steps))\n",
    "\n",
    "    # --- Use modern torch.amp.GradScaler initialization ---\n",
    "    # scaler = GradScaler(enabled=(config.device.type == 'cuda')) # Old/Deprecated style\n",
    "    scaler = torch.amp.GradScaler(device_type='cuda', enabled=(config.device.type == 'cuda'))\n",
    "    logger.info(f\"GradScaler enabled: {scaler.is_enabled()}\")\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Get pad_token_id and vocab_size\n",
    "    pad_token_id = data_processor.teacher_pad_id\n",
    "    teacher_vocab_size = model.config.vocab_size # Assumes model.config reflects the potentially resized vocab\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        logger.error(\"CRITICAL: Teacher pad_token_id is None. Using ignore_index=-100.\")\n",
    "        pad_token_id = -100\n",
    "    else:\n",
    "        logger.info(f\"Using ignore_index={pad_token_id} for teacher loss.\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.teacher_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad() # Zero gradients at the start\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.teacher_epochs}\")):\n",
    "            try:\n",
    "                input_ids = batch[\"input_ids\"].to(config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "                labels = batch[\"labels\"].to(config.device)\n",
    "\n",
    "                # --- Input Validation (Keep as before) ---\n",
    "                min_id, max_id = torch.min(input_ids), torch.max(input_ids)\n",
    "                if max_id >= teacher_vocab_size or min_id < 0:\n",
    "                    # ... (error logging and skipping logic) ...\n",
    "                    logger.error(f\"!!!!! Invalid teacher input_ids in batch {batch_idx+1}: Range [{min_id.item()},{max_id.item()}], Vocab Size {teacher_vocab_size}. Skipping.\")\n",
    "                    # Optional: Decode for debugging\n",
    "                    if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                    continue\n",
    "\n",
    "                min_lbl, max_lbl = torch.min(labels), torch.max(labels)\n",
    "                if max_lbl >= teacher_vocab_size or (min_lbl < 0 and min_lbl != pad_token_id):\n",
    "                    # ... (error logging and skipping logic) ...\n",
    "                    logger.error(f\"!!!!! Invalid teacher labels in batch {batch_idx+1}: Range [{min_lbl.item()},{max_lbl.item()}], Vocab Size {teacher_vocab_size}, Pad ID {pad_token_id}. Skipping.\")\n",
    "                    if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                    continue\n",
    "                # --- End Input Validation ---\n",
    "\n",
    "                # Use autocast context manager\n",
    "                context = torch.amp.autocast(device_type=config.device.type, dtype=torch.float16, enabled=(config.device.type == 'cuda'))\n",
    "                with context:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                    loss = loss_fn(logits.view(-1, teacher_vocab_size), labels.view(-1))\n",
    "\n",
    "                    # Check loss value before scaling\n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                         logger.error(f\"NaN or Inf loss detected *before* scaling at Epoch {epoch+1}, Batch {batch_idx+1}! Skipping step.\")\n",
    "                         # Optionally reduce scaler scale aggressively or reset optimizer state if persistent\n",
    "                         # scaler.update(max(scaler.get_scale() / 4.0, 1e-4)) # Example reduction\n",
    "                         optimizer.zero_grad() # Zero grad for this cycle\n",
    "                         continue # Skip backward and step\n",
    "\n",
    "                    loss = loss / config.gradient_accumulation_steps # Scale loss for accumulation\n",
    "\n",
    "                # Scaler operations outside the autocast context\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                    # Unscale, clip, step, update\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    # Consider clipping *after* checking grad norm if issues persist\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scale_before_update = scaler.get_scale()\n",
    "                    scaler.update()\n",
    "                    scale_after_update = scaler.get_scale()\n",
    "                    optimizer.zero_grad() # Zero grad after successful step\n",
    "                    scheduler.step()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Log scale changes if scale was skipped/reduced\n",
    "                    if not scaler._found_inf_per_device(optimizer).all() and scale_after_update < scale_before_update :\n",
    "                         logger.warning(f\"Scaler reduced scale to {scale_after_update:.1f} at step {global_step} (Grad Norm: {grad_norm:.4f})\")\n",
    "                    elif scaler._found_inf_per_device(optimizer).any():\n",
    "                         logger.warning(f\"Scaler skipped step due to Inf/NaN grads at step {global_step} (Grad Norm: {grad_norm:.4f}). Scale still {scale_after_update:.1f}\")\n",
    "\n",
    "\n",
    "                # Accumulate unscaled loss for logging\n",
    "                # Use loss_to_log = loss.item() * config.gradient_accumulation_steps\n",
    "                # Check item() availability before calling\n",
    "                try:\n",
    "                     loss_item = loss.item()\n",
    "                     epoch_loss += loss_item * config.gradient_accumulation_steps\n",
    "                except Exception as item_err:\n",
    "                     logger.error(f\"Could not get loss.item() at batch {batch_idx+1}: {item_err}\")\n",
    "                     loss_item = float('nan')\n",
    "\n",
    "\n",
    "                if (batch_idx + 1) % 50 == 0: # Log every 50 micro-batches\n",
    "                    step_loss_log = loss_item * config.gradient_accumulation_steps if not np.isnan(loss_item) else float('nan')\n",
    "                    logger.info(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Step Loss: {step_loss_log:.4f}, Scale: {scaler.get_scale():.1f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                 if \"CUDA out of memory\" in str(e):\n",
    "                     logger.error(f\"CUDA OOM error during training batch {batch_idx+1}. Consider reducing batch size further or enabling gradient checkpointing.\")\n",
    "                     if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                     # VERY IMPORTANT: Reset accumulation if OOM happens mid-cycle\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue # Skip batch\n",
    "                 else:\n",
    "                     logger.error(f\"RuntimeError during training batch {batch_idx+1}: {e}\", exc_info=True)\n",
    "                     if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue # Skip batch on other runtime errors\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during training batch {batch_idx+1}: {e}\", exc_info=True)\n",
    "                if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                continue\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        # Perform final optimizer step if accumulation ended mid-epoch\n",
    "        if (len(train_loader) % config.gradient_accumulation_steps) != 0:\n",
    "             logger.info(\"Performing final optimizer step for remaining accumulated gradients.\")\n",
    "             try:\n",
    "                 scaler.unscale_(optimizer)\n",
    "                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                 scaler.step(optimizer)\n",
    "                 scaler.update()\n",
    "                 scheduler.step() # Step scheduler here too\n",
    "                 optimizer.zero_grad()\n",
    "                 global_step += 1\n",
    "             except Exception as final_step_e:\n",
    "                 logger.error(f\"Error during final optimizer step: {final_step_e}\")\n",
    "\n",
    "\n",
    "        # Calculate average epoch loss based on successful steps\n",
    "        avg_epoch_loss = epoch_loss / global_step if global_step > 0 else float('nan')\n",
    "\n",
    "        # Validation - Pass the correct pad_token_id\n",
    "        # Ensure evaluate_teacher is defined and handles potential NaNs correctly\n",
    "        try:\n",
    "             val_loss, val_perplexity, val_accuracy = evaluate_teacher(model, val_loader, config, pad_token_id)\n",
    "        except Exception as eval_e:\n",
    "             logger.error(f\"Error during teacher evaluation: {eval_e}\", exc_info=True)\n",
    "             val_loss, val_perplexity, val_accuracy = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1} completed. Avg Train Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Checkpointing and Early Stopping (Keep as before) ---\n",
    "        # ... (logic based on val_loss) ...\n",
    "        if not np.isnan(val_loss) and val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, \"best_teacher_model.pt\"))\n",
    "            logger.info(f\"New best teacher model saved with val loss: {best_loss:.4f}\")\n",
    "        elif not np.isnan(val_loss):\n",
    "            early_stopping_counter += 1\n",
    "            logger.info(f\"Validation loss did not improve for {early_stopping_counter} epoch(s). Best loss: {best_loss:.4f}\")\n",
    "            if early_stopping_counter >= config.early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else: # Handle NaN validation loss\n",
    "            logger.warning(f\"Validation loss is NaN at epoch {epoch+1}.\")\n",
    "            early_stopping_counter += 1\n",
    "            logger.info(f\"NaN validation loss encountered. Early stopping counter: {early_stopping_counter}/{config.early_stopping_patience}\")\n",
    "            if early_stopping_counter >= config.early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered due to persistent NaN validation loss.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Load best model weights before returning\n",
    "    best_model_path = os.path.join(config.model_dir, \"best_teacher_model.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            map_location = config.device\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=map_location))\n",
    "            logger.info(f\"Loaded best teacher model weights from {best_model_path}.\")\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error loading best teacher model weights: {e}\")\n",
    "             logger.warning(\"Returning model from last trained epoch state due to load error.\")\n",
    "    else:\n",
    "        logger.warning(\"No best teacher model checkpoint found. Returning model from last epoch.\")\n",
    "\n",
    "    # Disable gradient checkpointing before returning if it was enabled\n",
    "    if hasattr(model, 'bert_model') and config.device.type == 'cuda':\n",
    "         try:\n",
    "             # Check if gradient checkpointing is actually enabled before trying to disable\n",
    "             if getattr(model.bert_model.config, \"use_cache\", True) is False: # Common way transformers disables cache for GC\n",
    "                 model.bert_model.gradient_checkpointing_disable()\n",
    "                 logger.info(\"Gradient Checkpointing Disabled on model.bert_model.\")\n",
    "         except Exception as e:\n",
    "             logger.warning(f\"Could not disable gradient checkpointing: {e}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab0f8778-c01f-4e06-adce-cc5703d8739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: evaluate_teacher (Added ignore_index and pad_token_id parameter)\n",
    "def evaluate_teacher(model, val_loader, config, pad_token_id): # Added pad_token_id parameter\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # *** FIX: Define loss function with ignore_index ***\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id if pad_token_id is not None else -100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating Teacher\"):\n",
    "            input_ids = batch[\"input_ids\"].to(config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "            labels = batch[\"labels\"].to(config.device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=config.device.type, enabled=(config.device.type == 'cuda')):\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                # Calculate loss using pre-defined function\n",
    "                loss = loss_fn(logits.view(-1, model.vocab_size), labels.view(-1))\n",
    "\n",
    "            if not torch.isnan(loss): # Only accumulate valid losses\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                logger.warning(\"NaN loss encountered during teacher evaluation.\")\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            targets = labels.cpu().numpy()\n",
    "\n",
    "            # *** FIX: Exclude padding tokens from accuracy calculation ***\n",
    "            # Create a mask for non-padding tokens in the labels\n",
    "            valid_indices = (targets != pad_token_id) if pad_token_id is not None else np.ones_like(targets, dtype=bool)\n",
    "\n",
    "            # Flatten and apply mask\n",
    "            all_preds.extend(preds.flatten()[valid_indices.flatten()])\n",
    "            all_targets.extend(targets.flatten()[valid_indices.flatten()])\n",
    "\n",
    "    # Calculate metrics, checking for division by zero or empty lists\n",
    "    num_batches = len(val_loader)\n",
    "    num_targets = len(all_targets)\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 and not np.isnan(total_loss) else float('nan')\n",
    "    perplexity = np.exp(avg_loss) if not np.isnan(avg_loss) else float('nan')\n",
    "    accuracy = accuracy_score(all_targets, all_preds) if num_targets > 0 else 0.0\n",
    "\n",
    "    return avg_loss, perplexity, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4271129-4680-4b2b-917f-f2e12d8be099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:27:34,126 - __main__ - INFO - bitsandbytes found, will use AdamW8bit.\n"
     ]
    }
   ],
   "source": [
    "# Cell 228: train_teacher (Remove Grad Checkpointing, Fix GradScaler Init, Keep 8-bit AdamW)\n",
    "\n",
    "# Ensure necessary imports are available\n",
    "from torch.cuda.amp import GradScaler # Using the import from Cell 5\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW # Keep standard AdamW import for fallback if needed\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Attempt to import bitsandbytes for 8-bit AdamW\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    bnb_available = True\n",
    "    logger.info(\"bitsandbytes found, will use AdamW8bit.\")\n",
    "except ImportError:\n",
    "    logger.warning(\"bitsandbytes not found. Falling back to standard AdamW. \"\n",
    "                   \"Install with 'pip install bitsandbytes' for memory savings.\")\n",
    "    bnb_available = False\n",
    "\n",
    "logger = logging.getLogger(__name__) # Ensure logger is defined\n",
    "\n",
    "\n",
    "def train_teacher(model, train_loader, val_loader, config, data_processor): # Added data_processor\n",
    "    logger.info(\"Starting teacher model fine-tuning for Hindi...\")\n",
    "    model.to(config.device)\n",
    "\n",
    "    # --- REMOVED Gradient Checkpointing attempt as ALBERT model doesn't support it ---\n",
    "    # if hasattr(model, 'bert_model') and config.device.type == 'cuda':\n",
    "    #      try:\n",
    "    #          model.bert_model.gradient_checkpointing_enable()\n",
    "    #          logger.info(\"Gradient Checkpointing Enabled on model.bert_model.\")\n",
    "    #      except AttributeError:\n",
    "    #           logger.warning(\"model.bert_model does not have gradient_checkpointing_enable. Skipping.\")\n",
    "    #      except Exception as e:\n",
    "    #           logger.error(f\"Error enabling gradient checkpointing: {e}. Skipping.\")\n",
    "    # elif config.device.type == 'cuda':\n",
    "    #      logger.warning(\"Gradient checkpointing requested but model object doesn't have 'bert_model' attribute.\")\n",
    "    logger.info(\"Gradient Checkpointing skipped (model does not support it or not requested).\")\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # --- Use 8-bit AdamW if available, otherwise standard AdamW ---\n",
    "    if bnb_available:\n",
    "        optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    else:\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    # ----------------------------------------------------------------\n",
    "\n",
    "    # Adjust total_steps for gradient accumulation\n",
    "    num_update_steps_per_epoch = len(train_loader) // config.gradient_accumulation_steps if len(train_loader) > 0 else 0\n",
    "    total_steps = num_update_steps_per_epoch * config.teacher_epochs\n",
    "    num_warmup_steps = min(config.warmup_steps, total_steps // 10) if total_steps > 0 else 0\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=max(1, total_steps))\n",
    "\n",
    "    # --- FIX: Revert GradScaler initialization to match the import ---\n",
    "    # scaler = torch.amp.GradScaler(device_type='cuda', enabled=(config.device.type == 'cuda')) # Caused TypeError\n",
    "    scaler = GradScaler(enabled=(config.device.type == 'cuda')) # Matches `from torch.cuda.amp import GradScaler`\n",
    "    logger.info(f\"GradScaler enabled: {scaler.is_enabled()}\")\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    # Get pad_token_id and vocab_size\n",
    "    pad_token_id = data_processor.teacher_pad_id\n",
    "    teacher_vocab_size = model.config.vocab_size # Assumes model.config reflects the potentially resized vocab\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        logger.error(\"CRITICAL: Teacher pad_token_id is None. Using ignore_index=-100.\")\n",
    "        pad_token_id = -100\n",
    "    else:\n",
    "        logger.info(f\"Using ignore_index={pad_token_id} for teacher loss.\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.teacher_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad() # Zero gradients at the start\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.teacher_epochs}\")):\n",
    "            try:\n",
    "                # --- Data Loading and Validation (Keep as before) ---\n",
    "                input_ids = batch[\"input_ids\"].to(config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "                labels = batch[\"labels\"].to(config.device)\n",
    "\n",
    "                # --- Input Validation (Keep as before) ---\n",
    "                min_id, max_id = torch.min(input_ids), torch.max(input_ids)\n",
    "                if max_id >= teacher_vocab_size or min_id < 0:\n",
    "                     logger.error(f\"!!!!! Invalid teacher input_ids in batch {batch_idx+1}: Range [{min_id.item()},{max_id.item()}], Vocab Size {teacher_vocab_size}. Skipping.\")\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue\n",
    "\n",
    "                min_lbl, max_lbl = torch.min(labels), torch.max(labels)\n",
    "                if max_lbl >= teacher_vocab_size or (min_lbl < 0 and min_lbl != pad_token_id):\n",
    "                     logger.error(f\"!!!!! Invalid teacher labels in batch {batch_idx+1}: Range [{min_lbl.item()},{max_lbl.item()}], Vocab Size {teacher_vocab_size}, Pad ID {pad_token_id}. Skipping.\")\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue\n",
    "                # --- End Input Validation ---\n",
    "\n",
    "                # Use autocast context manager\n",
    "                context = torch.amp.autocast(device_type=config.device.type, dtype=torch.float16, enabled=(config.device.type == 'cuda'))\n",
    "                with context:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                    loss = loss_fn(logits.view(-1, teacher_vocab_size), labels.view(-1))\n",
    "\n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                         logger.error(f\"NaN or Inf loss detected *before* scaling at Epoch {epoch+1}, Batch {batch_idx+1}! Skipping step.\")\n",
    "                         # Resetting scaler state when NaN/Inf loss occurs before scaling\n",
    "                         # We need to re-initialize GradScaler here if we want to reset its state,\n",
    "                         # or just skip the update for this step. Skipping is simpler.\n",
    "                         # scaler = GradScaler(init_scale=scaler.get_scale() / 2.0, enabled=(config.device.type == 'cuda')) # Example Re-init\n",
    "                         optimizer.zero_grad()\n",
    "                         continue\n",
    "\n",
    "                    loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "                # Scaler operations\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer) # This step might be skipped internally if grads are inf/nan\n",
    "                    scale_before_update = scaler.get_scale()\n",
    "                    scaler.update() # Updates scale; decreases if inf/nan found, increases if stable\n",
    "                    scale_after_update = scaler.get_scale()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Log scale changes (Adjusted condition for clarity)\n",
    "                    # step_skipped = scaler._found_inf_per_device(optimizer).any()\n",
    "                    # if step_skipped:\n",
    "                    #     logger.warning(f\"Scaler skipped step due to Inf/NaN grads at step {global_step} (Grad Norm: {grad_norm:.4f}). Scale remains {scale_after_update:.1f}\")\n",
    "                    # elif scale_after_update < scale_before_update:\n",
    "                    #     logger.warning(f\"Scaler reduced scale to {scale_after_update:.1f} at step {global_step} (Grad Norm: {grad_norm:.4f})\")\n",
    "\n",
    "                    if scale_after_update < scale_before_update:\n",
    "                         # This implies the step might have been skipped or scale was reduced.\n",
    "                         logger.warning(f\"Scaler reduced scale from {scale_before_update:.1f} to {scale_after_update:.1f} at step {global_step} (Grad Norm: {grad_norm:.4f}). Potential instability or skipped step.\")\n",
    "                    # else: # Optional: Log normal updates\n",
    "                        # logger.info(f\"Scale updated to {scale_after_update:.1f}, Grad Norm: {grad_norm:.4f}\")\n",
    "\n",
    "                # Accumulate loss for logging\n",
    "                try:\n",
    "                     loss_item = loss.item()\n",
    "                     epoch_loss += loss_item * config.gradient_accumulation_steps\n",
    "                except Exception as item_err:\n",
    "                     logger.error(f\"Could not get loss.item() at batch {batch_idx+1}: {item_err}\")\n",
    "                     loss_item = float('nan')\n",
    "\n",
    "                if (batch_idx + 1) % 50 == 0: # Log every 50 micro-batches\n",
    "                    step_loss_log = loss_item * config.gradient_accumulation_steps if not np.isnan(loss_item) else float('nan')\n",
    "                    logger.info(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Step Loss: {step_loss_log:.4f}, Scale: {scaler.get_scale():.1f}\")\n",
    "\n",
    "            # --- Exception Handling (Keep as before) ---\n",
    "            except RuntimeError as e:\n",
    "                 if \"CUDA out of memory\" in str(e):\n",
    "                     logger.error(f\"CUDA OOM error during training batch {batch_idx+1}. Memory saving options in use: 8-bit Adam={'Yes' if bnb_available else 'No'}, Grad Checkpointing=No. Consider reducing batch_size further.\")\n",
    "                     if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue\n",
    "                 else:\n",
    "                     logger.error(f\"RuntimeError during training batch {batch_idx+1}: {e}\", exc_info=True)\n",
    "                     if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                     if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                     continue\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during training batch {batch_idx+1}: {e}\", exc_info=True)\n",
    "                if config.device.type == 'cuda': torch.cuda.empty_cache()\n",
    "                if (batch_idx + 1) % config.gradient_accumulation_steps != 0: optimizer.zero_grad()\n",
    "                continue\n",
    "        # --- End Batch Loop ---\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        # Final optimizer step if needed\n",
    "        if (len(train_loader) % config.gradient_accumulation_steps) != 0:\n",
    "             logger.info(\"Performing final optimizer step for remaining accumulated gradients.\")\n",
    "             try:\n",
    "                 scaler.unscale_(optimizer)\n",
    "                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                 scaler.step(optimizer)\n",
    "                 scaler.update()\n",
    "                 scheduler.step()\n",
    "                 optimizer.zero_grad()\n",
    "                 global_step += 1 # Ensure global step increments if final step happens\n",
    "             except Exception as final_step_e:\n",
    "                 logger.error(f\"Error during final optimizer step: {final_step_e}\")\n",
    "\n",
    "        # Calculate average loss based on actual optimizer steps (global_step)\n",
    "        avg_epoch_loss = epoch_loss / global_step if global_step > 0 else float('nan')\n",
    "\n",
    "        # Validation\n",
    "        try:\n",
    "             # Ensure evaluate_teacher uses pad_token_id\n",
    "             val_loss, val_perplexity, val_accuracy = evaluate_teacher(model, val_loader, config, pad_token_id)\n",
    "        except Exception as eval_e:\n",
    "             logger.error(f\"Error during teacher evaluation: {eval_e}\", exc_info=True)\n",
    "             val_loss, val_perplexity, val_accuracy = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1} completed. Avg Train Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Checkpointing and Early Stopping (Keep as before) ---\n",
    "        if not np.isnan(val_loss) and val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, \"best_teacher_model.pt\"))\n",
    "            logger.info(f\"New best teacher model saved with val loss: {best_loss:.4f}\")\n",
    "        elif not np.isnan(val_loss):\n",
    "            early_stopping_counter += 1\n",
    "            logger.info(f\"Validation loss did not improve for {early_stopping_counter} epoch(s). Best loss: {best_loss:.4f}\")\n",
    "            if early_stopping_counter >= config.early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        else: # Handle NaN validation loss\n",
    "            logger.warning(f\"Validation loss is NaN at epoch {epoch+1}.\")\n",
    "            early_stopping_counter += 1\n",
    "            logger.info(f\"NaN validation loss encountered. Early stopping counter: {early_stopping_counter}/{config.early_stopping_patience}\")\n",
    "            if early_stopping_counter >= config.early_stopping_patience:\n",
    "                logger.info(f\"Early stopping triggered due to persistent NaN validation loss.\")\n",
    "                break\n",
    "    # --- End Epoch Loop ---\n",
    "\n",
    "    # Load best model weights before returning\n",
    "    best_model_path = os.path.join(config.model_dir, \"best_teacher_model.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            map_location = config.device\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=map_location))\n",
    "            logger.info(f\"Loaded best teacher model weights from {best_model_path}.\")\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error loading best teacher model weights: {e}\")\n",
    "             logger.warning(\"Returning model from last trained epoch state due to load error.\")\n",
    "    else:\n",
    "        logger.warning(\"No best teacher model checkpoint found. Returning model from last epoch.\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b4be45-2f33-4510-ac6a-1cc133d80929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 231: evaluate_student (Fix ignore_index, metrics calculation)\n",
    "# Ensure imports: torch, nn, tqdm, logger, np, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Cell 231: evaluate_student (Add shape debugging and checks)\n",
    "\n",
    "# Ensure necessary imports are available\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "logger = logging.getLogger(__name__) # Ensure logger is defined\n",
    "\n",
    "def evaluate_student(model, val_loader, config, data_processor): # Added data_processor\n",
    "    logger.info(\"Evaluating student model...\")\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Get student pad token ID for loss and metric calculation\n",
    "    try:\n",
    "        student_pad_id = data_processor.student_tokenizer.pad_token_id\n",
    "        student_vocab_size = model.vocab_size # Assumes model has this correctly set\n",
    "    except AttributeError as ae:\n",
    "         logger.error(f\"Error accessing attributes from data_processor or model in evaluate_student: {ae}\")\n",
    "         # Fallback or raise error - critical if pad_id/vocab_size are needed\n",
    "         student_pad_id = -100 # Fallback ignore index\n",
    "         try:\n",
    "             student_vocab_size = model.config.vocab_size # Try getting from config\n",
    "         except:\n",
    "              logger.error(\"Cannot determine student vocab size for evaluation!\")\n",
    "              raise ValueError(\"Cannot determine student vocab size.\")\n",
    "\n",
    "\n",
    "    if student_pad_id is None:\n",
    "        logger.warning(\"Student pad_token_id is None during evaluation. Using ignore_index=-100.\")\n",
    "        student_pad_id = -100\n",
    "    else:\n",
    "         logger.info(f\"Evaluation using student pad_token_id: {student_pad_id}\")\n",
    "\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    all_preds_eval = []\n",
    "    all_labels_eval = []\n",
    "\n",
    "    # Define loss function with ignore_index for evaluation\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=student_pad_id)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating Student\")):\n",
    "            try:\n",
    "                # --- 1. Data Loading and Basic Checks ---\n",
    "                if \"input_ids\" not in batch or \"attention_mask\" not in batch or \"labels\" not in batch:\n",
    "                    logger.warning(f\"Skipping eval batch {batch_idx} due to missing keys.\")\n",
    "                    continue\n",
    "\n",
    "                input_ids = batch[\"input_ids\"].to(config.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "                labels = batch[\"labels\"].to(config.device)\n",
    "\n",
    "                # --- 2. Shape Debugging and Validation ---\n",
    "                logger.debug(f\"Eval Batch {batch_idx}: input_ids shape {input_ids.shape}, attention_mask shape {attention_mask.shape}, labels shape {labels.shape}\")\n",
    "\n",
    "                # Check for consistent batch size and sequence length across inputs\n",
    "                batch_size = input_ids.shape[0]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "                if batch_size == 0:\n",
    "                    logger.warning(f\"Skipping eval batch {batch_idx} due to zero batch size.\")\n",
    "                    continue\n",
    "\n",
    "                if attention_mask.shape != (batch_size, seq_len):\n",
    "                    logger.warning(f\"Skipping eval batch {batch_idx} due to inconsistent attention_mask shape: {attention_mask.shape}\")\n",
    "                    continue\n",
    "                # >>> CRITICAL CHECK for the ValueError <<<\n",
    "                if labels.shape != (batch_size, seq_len):\n",
    "                    logger.error(f\"CRITICAL SHAPE MISMATCH in eval batch {batch_idx}: Labels shape is {labels.shape}, but expected ({batch_size}, {seq_len}).\")\n",
    "                    logger.error(\"This is likely the cause of the ValueError. Check FewShotDataset or DataLoader collation for 'labels'. Skipping batch.\")\n",
    "                    continue # Skip this batch as loss calculation will fail\n",
    "\n",
    "                # --- 3. Forward Pass ---\n",
    "                context = torch.amp.autocast(device_type=config.device.type, dtype=torch.float16, enabled=(config.device.type == 'cuda'))\n",
    "                with context:\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "\n",
    "                # Check logits shape\n",
    "                expected_logit_shape = (batch_size, seq_len, student_vocab_size)\n",
    "                if logits.shape != expected_logit_shape:\n",
    "                     logger.error(f\"Eval Batch {batch_idx}: Unexpected logits shape. Got {logits.shape}, Expected {expected_logit_shape}. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                # --- 4. Loss Calculation ---\n",
    "                # Reshape for loss - dimensions should now match if shapes were correct above\n",
    "                logits_view = logits.view(-1, student_vocab_size) # Shape: (batch_size * seq_len, vocab_size)\n",
    "                labels_view = labels.view(-1) # Shape: (batch_size * seq_len)\n",
    "\n",
    "                # Double check before loss - this should not fail if previous check passed\n",
    "                if logits_view.shape[0] != labels_view.shape[0]:\n",
    "                     logger.error(f\"UNEXPECTED shape mismatch right before loss in eval batch {batch_idx}: \"\n",
    "                                  f\"Logits_view[0]={logits_view.shape[0]}, Labels_view[0]={labels_view.shape[0]}. Skipping loss.\")\n",
    "                     loss = torch.tensor(float('nan')) # Assign NaN\n",
    "                else:\n",
    "                     loss = loss_fn(logits_view, labels_view)\n",
    "\n",
    "\n",
    "                # Accumulate loss (only if valid)\n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    total_loss += loss.item()\n",
    "                    total_batches += 1\n",
    "                else:\n",
    "                     logger.warning(f\"NaN or Inf loss calculated during student evaluation batch {batch_idx}.\")\n",
    "\n",
    "\n",
    "                # --- 5. Store Predictions for Metrics ---\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                valid_mask = (labels != student_pad_id)\n",
    "                all_preds_eval.extend(preds[valid_mask].cpu().tolist())\n",
    "                all_labels_eval.extend(labels[valid_mask].cpu().tolist())\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during student evaluation batch {batch_idx}: {e}\", exc_info=True)\n",
    "                continue # Skip batch on error\n",
    "\n",
    "    # --- Calculate final metrics (Keep as before) ---\n",
    "    if total_batches > 0:\n",
    "        avg_loss = total_loss / total_batches\n",
    "        perplexity = np.exp(avg_loss) if not np.isnan(avg_loss) and not np.isinf(avg_loss) else float('inf')\n",
    "    else:\n",
    "        avg_loss = float('inf')\n",
    "        perplexity = float('inf')\n",
    "        logger.warning(\"No valid batches processed during student evaluation.\")\n",
    "\n",
    "    if all_labels_eval:\n",
    "        accuracy = accuracy_score(all_labels_eval, all_preds_eval)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels_eval, all_preds_eval, average='weighted', zero_division=0\n",
    "        )\n",
    "    else:\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        logger.warning(\"No valid (non-padding) labels found during student evaluation to calculate metrics.\")\n",
    "\n",
    "    results = {\n",
    "        \"loss\": avg_loss if not np.isinf(avg_loss) else float('inf'), # Ensure inf is returned if avg_loss is inf\n",
    "        \"perplexity\": perplexity,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "    logger.info(\"========== Student Model Evaluation Results ==========\")\n",
    "    logger.info(f\"  Loss       : {results['loss']:.4f}\")\n",
    "    logger.info(f\"  Perplexity : {results['perplexity']:.4f}\")\n",
    "    logger.info(f\"  Accuracy   : {results['accuracy']:.4f}\")\n",
    "    logger.info(f\"  Precision  : {results['precision']:.4f}\")\n",
    "    logger.info(f\"  Recall     : {results['recall']:.4f}\")\n",
    "    logger.info(f\"  F1 Score   : {results['f1']:.4f}\")\n",
    "    logger.info(\"====================================================\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b6cbcc4-bf24-40be-8d69-8cd6964f6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(metrics_history, config):\n",
    "    \"\"\"Plot training and validation metrics history\"\"\"\n",
    "    epochs = [m['epoch'] for m in metrics_history]\n",
    "    \n",
    "    # Create directory for plots\n",
    "    plots_dir = os.path.join(config.output_dir, \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot 1: Train vs Val Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, [m['train_accuracy'] for m in metrics_history], 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, [m['val_accuracy'] for m in metrics_history], 'r-', label='Val Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'accuracy.png'))\n",
    "    \n",
    "    # Plot 2: Train Loss and Reward\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Loss on left y-axis\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='tab:red')\n",
    "    ax1.plot(epochs, [m['train_loss'] for m in metrics_history], 'tab:red', label='Train Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Reward on right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Reward', color='tab:blue')\n",
    "    ax2.plot(epochs, [m['train_reward'] for m in metrics_history], 'tab:blue', label='Train Reward')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title('Training Loss and Reward over Epochs')\n",
    "    plt.savefig(os.path.join(plots_dir, 'loss_reward.png'))\n",
    "    \n",
    "    # Plot 3: Validation Metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, [m['val_accuracy'] for m in metrics_history], 'g-', label='Accuracy')\n",
    "    plt.plot(epochs, [m['val_precision'] for m in metrics_history], 'b-', label='Precision')\n",
    "    plt.plot(epochs, [m['val_recall'] for m in metrics_history], 'r-', label='Recall')\n",
    "    plt.plot(epochs, [m['val_f1'] for m in metrics_history], 'y-', label='F1')\n",
    "    plt.title('Validation Metrics over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'val_metrics.png'))\n",
    "    \n",
    "    # Save metrics history as CSV\n",
    "    pd.DataFrame(metrics_history).to_csv(os.path.join(config.output_dir, 'metrics_history.csv'), index=False)\n",
    "    \n",
    "    logger.info(f\"Training history plots saved to {plots_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b7ca10d-08ef-46b1-a1f3-86bd3a237eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 233: main function (Rewritten - Fix All Identified Errors)\n",
    "\n",
    "# --- Imports ---\n",
    "# Ensure all necessary libraries and your custom modules/classes are imported\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, AutoConfig, AutoModelForMaskedLM # Added Auto* imports if needed here\n",
    "import os\n",
    "import numpy as np\n",
    "import traceback # For more detailed error logging if needed\n",
    "\n",
    "# Import your classes/functions if they are not auto-imported\n",
    "# (Assuming they are defined in previous cells or imported correctly)\n",
    "# E.g.: from your_module import Config, DataProcessor, TeacherModel, StudentModel, train_teacher, evaluate_teacher, train_rl, evaluate_student, plot_training_history, NextWordPredictionDataset, FewShotDataset, validate_dataset\n",
    "\n",
    "# --- Logger Setup ---\n",
    "# Make sure logger is configured correctly in an earlier cell or here\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "# --- Main Function Definition ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the teacher-student training pipeline.\n",
    "    \"\"\"\n",
    "    config = Config()\n",
    "    logger.info(f\"Configuration loaded. Using device: {config.device}\")\n",
    "    logger.info(f\"Teacher model: {config.teacher_model_name}, Student model: {config.student_model_name}\")\n",
    "    logger.info(f\"Max length: {config.max_length}, Batch size: {config.batch_size}, Accumulation: {config.gradient_accumulation_steps}\")\n",
    "\n",
    "    # --- 1. Initialize Data Processor ---\n",
    "    logger.info(\"Initializing DataProcessor...\")\n",
    "    try:\n",
    "        # Ensure DataProcessor class is defined and handles tokenizer loading\n",
    "        data_processor = DataProcessor(config)\n",
    "        teacher_tokenizer = data_processor.teacher_tokenizer\n",
    "        student_tokenizer = data_processor.student_tokenizer\n",
    "        logger.info(f\"Teacher vocab size: {data_processor.teacher_vocab_size}, Student vocab size: {data_processor.student_vocab_size}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize DataProcessor: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 2. Initialize Teacher Model ON CPU FIRST ---\n",
    "    logger.info(\"Initializing teacher model on CPU...\")\n",
    "    try:\n",
    "        # Ensure TeacherModel class is defined\n",
    "        # --- FIX: Pass correct arguments instead of Ellipsis ---\n",
    "        teacher_model = TeacherModel(\n",
    "            config.teacher_model_name,\n",
    "            tokenizer_vocab_size=data_processor.teacher_vocab_size\n",
    "        )\n",
    "        # -----------------------------------------------------\n",
    "        logger.info(f\"Teacher model initialized on CPU. Model config vocab size: {teacher_model.config.vocab_size}\")\n",
    "        # --- Optional CPU Test ---\n",
    "        # logger.info(\"Performing quick CPU forward pass test...\")\n",
    "        # ... (CPU test logic) ...\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize teacher model: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 3. Move Teacher Model to GPU ---\n",
    "    logger.info(f\"Attempting to move teacher model to device: {config.device}...\")\n",
    "    try:\n",
    "        teacher_model.to(config.device)\n",
    "        actual_device = next(teacher_model.parameters()).device\n",
    "        logger.info(f\"Teacher model successfully moved. Device check: {actual_device}\")\n",
    "        if actual_device != config.device:\n",
    "             logger.warning(f\"Teacher model parameters are on {actual_device}, but config device is {config.device}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to move teacher model to {config.device}: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Cannot proceed without moving teacher model to {config.device}\")\n",
    "\n",
    "    # --- 4. Load Datasets ---\n",
    "    logger.info(\"Loading datasets...\")\n",
    "    try:\n",
    "        hindi_samples, chhattisgarhi_samples = data_processor.load_dataset()\n",
    "        if not hindi_samples or not chhattisgarhi_samples:\n",
    "             raise ValueError(\"Dataset loading returned empty lists.\")\n",
    "        logger.info(f\"Loaded {len(hindi_samples)} Hindi and {len(chhattisgarhi_samples)} Chhattisgarhi samples.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load datasets: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 5. Teacher Data Setup & Validation ---\n",
    "    logger.info(\"Creating teacher (Hindi) dataset and dataloaders...\")\n",
    "    try:\n",
    "        # Ensure NextWordPredictionDataset class is defined\n",
    "        hindi_dataset = NextWordPredictionDataset(hindi_samples, teacher_tokenizer, max_length=config.max_length)\n",
    "        logger.info(f\"Hindi dataset created with {len(hindi_dataset)} examples after filtering.\")\n",
    "        if len(hindi_dataset) == 0: raise ValueError(\"Hindi NextWordPredictionDataset is empty!\")\n",
    "\n",
    "        train_size_h = int((1.0 - config.train_test_split) * len(hindi_dataset))\n",
    "        val_size_h = len(hindi_dataset) - train_size_h\n",
    "        if train_size_h <= 0 or val_size_h <= 0: raise ValueError(\"Hindi train/val split resulted in empty dataset.\")\n",
    "        hindi_train_dataset, hindi_val_dataset = random_split(hindi_dataset, [train_size_h, val_size_h], generator=torch.Generator().manual_seed(42))\n",
    "        logger.info(f\"Hindi dataset sizes: Train={len(hindi_train_dataset)}, Val={len(hindi_val_dataset)}\")\n",
    "\n",
    "        # Ensure validate_dataset function is defined\n",
    "        if not validate_dataset(hindi_train_dataset, teacher_tokenizer, name=\"Hindi Train\"): raise ValueError(\"Validation failed for Hindi training data.\")\n",
    "        if not validate_dataset(hindi_val_dataset, teacher_tokenizer, name=\"Hindi Val\"): raise ValueError(\"Validation failed for Hindi validation data.\")\n",
    "\n",
    "        # DataLoader instantiation with correct keyword arguments\n",
    "        hindi_train_loader = DataLoader(\n",
    "            hindi_train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if config.device.type == 'cuda' else False\n",
    "        )\n",
    "        hindi_val_loader = DataLoader(\n",
    "            hindi_val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if config.device.type == 'cuda' else False\n",
    "        )\n",
    "        logger.info(\"Hindi DataLoaders created.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Hindi data pipeline: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 6. Fine-tune Teacher Model ---\n",
    "    logger.info(\"Starting teacher model fine-tuning...\")\n",
    "    try:\n",
    "        # Ensure train_teacher is defined (Cell 228) and run\n",
    "        if 'train_teacher' not in globals(): raise NameError(\"Function 'train_teacher' is not defined.\")\n",
    "        teacher_model = train_teacher(teacher_model, hindi_train_loader, hindi_val_loader, config, data_processor)\n",
    "        logger.info(\"Teacher model fine-tuning complete.\")\n",
    "    except NameError as ne:\n",
    "         logger.error(f\"NameError during teacher training: {ne}. Have you run the cell defining 'train_teacher'?\")\n",
    "         raise ne\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during teacher model training: {e}\", exc_info=True)\n",
    "        raise # Stop if teacher training fails\n",
    "\n",
    "    # --- 7. Student Data Setup & Validation ---\n",
    "    logger.info(\"Creating student (Chhattisgarhi) dataset and dataloaders...\")\n",
    "    try:\n",
    "        # Ensure FewShotDataset class is defined\n",
    "        chhattisgarhi_dataset = FewShotDataset(chhattisgarhi_samples, student_tokenizer, few_shot_examples=config.few_shot_examples, max_length=config.max_length)\n",
    "        logger.info(f\"Chhattisgarhi dataset created with {len(chhattisgarhi_dataset)} examples after filtering.\")\n",
    "        if len(chhattisgarhi_dataset) == 0: raise ValueError(\"Chhattisgarhi FewShotDataset is empty!\")\n",
    "\n",
    "        train_size_c = int((1.0 - config.train_test_split) * len(chhattisgarhi_dataset))\n",
    "        val_size_c = len(chhattisgarhi_dataset) - train_size_c\n",
    "        if train_size_c <= 0 or val_size_c <= 0: raise ValueError(\"Chhattisgarhi train/val split resulted in empty dataset(s).\")\n",
    "        chhattisgarhi_train_dataset, chhattisgarhi_val_dataset = random_split(chhattisgarhi_dataset, [train_size_c, val_size_c], generator=torch.Generator().manual_seed(42))\n",
    "        logger.info(f\"Chhattisgarhi dataset sizes: Train={len(chhattisgarhi_train_dataset)}, Val={len(chhattisgarhi_val_dataset)}\")\n",
    "\n",
    "        # Validate datasets\n",
    "        if not validate_dataset(chhattisgarhi_train_dataset, student_tokenizer, name=\"Chhattisgarhi Train\"): raise ValueError(\"Validation failed for Chhattisgarhi training data.\")\n",
    "        if not validate_dataset(chhattisgarhi_val_dataset, student_tokenizer, name=\"Chhattisgarhi Val\"): raise ValueError(\"Validation failed for Chhattisgarhi validation data.\")\n",
    "\n",
    "        # --- Create Chhattisgarhi DataLoaders FIRST ---\n",
    "        chhattisgarhi_train_loader = DataLoader(\n",
    "            chhattisgarhi_train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if config.device.type == 'cuda' else False\n",
    "        )\n",
    "        chhattisgarhi_val_loader = DataLoader(\n",
    "            chhattisgarhi_val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if config.device.type == 'cuda' else False\n",
    "        )\n",
    "        logger.info(\"Chhattisgarhi DataLoaders created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Chhattisgarhi data pipeline: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- Student Data Sanity Check (Moved After DataLoader Creation) ---\n",
    "    logger.info(\"Sanity checking Chhattisgarhi training data labels...\")\n",
    "    try:\n",
    "        student_pad_id_check = data_processor.student_tokenizer.pad_token_id\n",
    "        if student_pad_id_check is None: student_pad_id_check = -100\n",
    "\n",
    "        num_batches_to_check = min(5, len(chhattisgarhi_train_loader))\n",
    "        if num_batches_to_check == 0:\n",
    "            logger.warning(\"Student train loader is empty, skipping sanity check.\")\n",
    "        else:\n",
    "            total_valid_labels = 0\n",
    "            unique_labels_found = set()\n",
    "            for i, batch in enumerate(chhattisgarhi_train_loader):\n",
    "                if i >= num_batches_to_check: break\n",
    "                if 'labels' not in batch:\n",
    "                     logger.warning(f\"Batch {i} missing 'labels' key during sanity check.\")\n",
    "                     continue\n",
    "                labels = batch['labels']\n",
    "                valid_mask = labels != student_pad_id_check\n",
    "                valid_labels_in_batch = labels[valid_mask]\n",
    "                total_valid_labels += len(valid_labels_in_batch)\n",
    "                unique_in_batch, _ = torch.unique(valid_labels_in_batch, return_counts=True)\n",
    "                unique_labels_found.update(unique_in_batch.tolist())\n",
    "\n",
    "            if total_valid_labels == 0:\n",
    "                logger.error(\"CRITICAL: No valid (non-padding) labels found in the first few batches of student training data! Check dataset creation.\")\n",
    "            else:\n",
    "                 logger.info(f\"Checked {num_batches_to_check} batches: Found {total_valid_labels} valid labels, {len(unique_labels_found)} unique label IDs.\")\n",
    "    except Exception as sanity_e:\n",
    "        logger.error(f\"Error during student data sanity check: {sanity_e}\", exc_info=True)\n",
    "    # --- END DATA SANITY CHECK ---\n",
    "\n",
    "\n",
    "    # --- 8. Student Model Init, Resize, GPU Move ---\n",
    "    logger.info(f\"Initializing student model: {config.student_model_name} on CPU...\")\n",
    "    try:\n",
    "        # Ensure StudentModel class is defined\n",
    "        # --- Ensure no Ellipsis here either ---\n",
    "        student_model = StudentModel(config.student_model_name)\n",
    "        # ------------------------------------\n",
    "\n",
    "        # Resize student model token embeddings *on CPU* if necessary\n",
    "        student_tokenizer_len = len(data_processor.student_tokenizer)\n",
    "        if student_model.model.config.vocab_size != student_tokenizer_len:\n",
    "            logger.warning(f\"Resizing student model token embeddings from {student_model.model.config.vocab_size} to {student_tokenizer_len}\")\n",
    "            student_model.model.resize_token_embeddings(student_tokenizer_len)\n",
    "            # IMPORTANT: Update vocab_size attribute and re-initialize head layer if StudentModel class doesn't handle this internally\n",
    "            if hasattr(student_model, 'vocab_size'):\n",
    "                 student_model.vocab_size = student_model.model.config.vocab_size\n",
    "            if hasattr(student_model, 'next_token_head'):\n",
    "                 student_model.next_token_head = nn.Linear(student_model.hidden_size, student_model.model.config.vocab_size)\n",
    "                 logger.info(\"Re-initialized student model head layer after resizing.\")\n",
    "            else:\n",
    "                 logger.warning(\"StudentModel does not have 'vocab_size' or 'next_token_head' attributes to update after resize.\")\n",
    "            logger.info(f\"Resized student model embeddings. New config vocab size: {student_model.model.config.vocab_size}\")\n",
    "        else:\n",
    "             logger.info(\"Student model vocab size already matches tokenizer.\")\n",
    "\n",
    "        # Move student model to GPU\n",
    "        student_model.to(config.device)\n",
    "        actual_device_student = next(student_model.parameters()).device\n",
    "        logger.info(f\"Student model initialized and moved to device. Device check: {actual_device_student}\")\n",
    "        if actual_device_student != config.device:\n",
    "             logger.warning(f\"Student model parameters are on {actual_device_student}, expected {config.device}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize or move student model: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 9. KD+CE Training Setup ---\n",
    "    logger.info(\"Setting up student optimizer and scheduler...\")\n",
    "    try:\n",
    "        # Ensure AdamW and get_linear_schedule_with_warmup are imported\n",
    "        optimizer = AdamW(student_model.parameters(), lr=config.rl_lr, weight_decay=config.weight_decay)\n",
    "        # Robust calculation for total steps\n",
    "        if len(chhattisgarhi_train_loader) == 0:\n",
    "             logger.warning(\"Student train loader is empty. Setting RL total steps based on epochs only.\")\n",
    "             rl_total_steps = config.distillation_epochs\n",
    "             num_rl_update_steps_per_epoch = 0\n",
    "        else:\n",
    "             num_rl_update_steps_per_epoch = max(1, len(chhattisgarhi_train_loader) // config.gradient_accumulation_steps)\n",
    "             rl_total_steps = num_rl_update_steps_per_epoch * config.distillation_epochs\n",
    "\n",
    "        rl_num_warmup_steps = min(config.warmup_steps, rl_total_steps // 10) if rl_total_steps > 0 else 0\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=rl_num_warmup_steps, num_training_steps=max(1, rl_total_steps)) # Ensure num_training_steps >= 1\n",
    "        logger.info(f\"Student (KD+CE) training setup: {rl_total_steps} total steps ({num_rl_update_steps_per_epoch} steps/epoch), {rl_num_warmup_steps} warmup steps.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to setup student optimizer/scheduler: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    # --- 10. KD+CE Training Loop ---\n",
    "    logger.info(\"Starting student model training with KD + CE...\")\n",
    "    metrics_history = []\n",
    "    best_val_accuracy = -1.0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Check if train loader has steps\n",
    "    if num_rl_update_steps_per_epoch == 0 and config.distillation_epochs > 0:\n",
    "         logger.error(\"Student training cannot proceed: train loader is empty or too small for gradient accumulation.\")\n",
    "    else:\n",
    "        for epoch in range(config.distillation_epochs):\n",
    "            logger.info(f\"--- Starting Student Epoch {epoch+1}/{config.distillation_epochs} ---\")\n",
    "            try:\n",
    "                # --- Ensure Cells 230 (train_rl) & 231 (evaluate_student) have been run ---\n",
    "                if 'train_rl' not in globals(): raise NameError(\"Function 'train_rl' is not defined.\")\n",
    "                if 'evaluate_student' not in globals(): raise NameError(\"Function 'evaluate_student' is not defined.\")\n",
    "\n",
    "                # Call training step\n",
    "                avg_loss, train_acc, train_prec, train_rec, train_f1 = train_rl(\n",
    "                    teacher_model, student_model, chhattisgarhi_train_loader, optimizer, scheduler, config, data_processor\n",
    "                )\n",
    "                logger.info(f\"Epoch {epoch+1} Train Stats: Loss={avg_loss:.4f}, Acc={train_acc:.4f}, F1={train_f1:.4f}\")\n",
    "\n",
    "                # Call evaluation step\n",
    "                eval_metrics = evaluate_student(student_model, chhattisgarhi_val_loader, config, data_processor)\n",
    "\n",
    "                # Collect metrics\n",
    "                current_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train_loss': avg_loss, 'train_accuracy': train_acc, 'train_precision': train_prec, 'train_recall': train_rec, 'train_f1': train_f1,\n",
    "                    'val_loss': eval_metrics.get('loss', float('nan')), 'val_perplexity': eval_metrics.get('perplexity', float('nan')),\n",
    "                    'val_accuracy': eval_metrics.get('accuracy', float('nan')), 'val_precision': eval_metrics.get('precision', float('nan')),\n",
    "                    'val_recall': eval_metrics.get('recall', float('nan')), 'val_f1': eval_metrics.get('f1', float('nan'))\n",
    "                }\n",
    "                metrics_history.append(current_metrics)\n",
    "                logger.info(f\"Epoch {epoch+1} Val Stats: Loss={current_metrics['val_loss']:.4f}, Acc={current_metrics['val_accuracy']:.4f}, F1={current_metrics['val_f1']:.4f}\")\n",
    "\n",
    "                # --- Checkpointing & Early Stopping ---\n",
    "                current_val_acc = current_metrics['val_accuracy']\n",
    "                is_current_acc_valid = not (current_val_acc is None or np.isnan(current_val_acc))\n",
    "\n",
    "                if is_current_acc_valid and current_val_acc > best_val_accuracy:\n",
    "                    best_val_accuracy = current_val_acc\n",
    "                    epochs_without_improvement = 0\n",
    "                    best_model_path = os.path.join(config.model_dir, \"best_student_model.pt\")\n",
    "                    torch.save(student_model.state_dict(), best_model_path)\n",
    "                    logger.info(f\"*** New best student model saved with validation accuracy: {best_val_accuracy:.4f} at epoch {epoch+1} ***\")\n",
    "                elif is_current_acc_valid:\n",
    "                    epochs_without_improvement += 1\n",
    "                    logger.info(f\"Validation accuracy ({current_val_acc:.4f}) did not improve for {epochs_without_improvement} epoch(s). Best accuracy: {best_val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "                    logger.warning(f\"Validation accuracy is invalid (NaN/None) at epoch {epoch+1}. Treating as no improvement ({epochs_without_improvement}).\")\n",
    "\n",
    "                if epochs_without_improvement >= config.early_stopping_patience:\n",
    "                    logger.info(f\"Early stopping triggered after epoch {epoch+1} due to no improvement for {config.early_stopping_patience} epochs.\")\n",
    "                    break\n",
    "                # --- End Checkpointing ---\n",
    "\n",
    "            except NameError as ne:\n",
    "                 logger.error(f\"NameError during student training epoch {epoch+1}: {ne}\")\n",
    "                 logger.error(\"!!! Please ensure the cells defining 'train_rl' (Cell 230) and 'evaluate_student' (Cell 231) have been executed before running this cell. !!!\")\n",
    "                 raise ne # Stop execution\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during student training epoch {epoch+1}: {e}\", exc_info=True)\n",
    "                logger.warning(\"Error occurred, breaking student training loop.\")\n",
    "                break # Break loop on error\n",
    "\n",
    "    # --- 11. Final Steps ---\n",
    "    logger.info(\"Student training loop finished.\")\n",
    "\n",
    "    # Plotting history\n",
    "    if metrics_history:\n",
    "        logger.info(\"Plotting training history...\")\n",
    "        try:\n",
    "            # Ensure plot_training_history function is defined\n",
    "            if 'plot_training_history' in globals():\n",
    "                plot_training_history(metrics_history, config)\n",
    "            else:\n",
    "                logger.warning(\"Function 'plot_training_history' not defined. Skipping plotting.\")\n",
    "        except Exception as plot_e:\n",
    "            logger.error(f\"Failed to plot training history: {plot_e}\")\n",
    "    else:\n",
    "        logger.warning(\"No metrics history recorded, skipping plotting.\")\n",
    "\n",
    "    # Final Evaluation\n",
    "    logger.info(\"Performing final evaluation...\")\n",
    "    final_metrics = {}\n",
    "    if 'evaluate_student' not in globals():\n",
    "         logger.error(\"Cannot perform final evaluation: 'evaluate_student' not defined.\")\n",
    "    else:\n",
    "        best_student_model_path = os.path.join(config.model_dir, \"best_student_model.pt\")\n",
    "        if os.path.exists(best_student_model_path):\n",
    "            logger.info(f\"Loading best student model from {best_student_model_path} for final evaluation...\")\n",
    "            try:\n",
    "                map_location = config.device\n",
    "                # Re-initialize a student model instance on CPU first for loading, then move\n",
    "                final_student_model = StudentModel(config.student_model_name)\n",
    "                # Handle potential resize mismatch if config changed\n",
    "                # ... (resize logic similar to above if needed) ...\n",
    "                final_student_model.load_state_dict(torch.load(best_student_model_path, map_location=torch.device('cpu'))) # Load to CPU first\n",
    "                final_student_model.to(config.device) # Move to target device\n",
    "                logger.info(\"Successfully loaded best student model.\")\n",
    "                final_metrics = evaluate_student(final_student_model, chhattisgarhi_val_loader, config, data_processor)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to load or evaluate best student model: {e}\", exc_info=True)\n",
    "                 logger.warning(\"Evaluating model from last trained epoch state instead.\")\n",
    "                 final_metrics = evaluate_student(student_model, chhattisgarhi_val_loader, config, data_processor) # Use model in memory\n",
    "        else:\n",
    "            logger.warning(\"Best student model checkpoint not found. Evaluating model from last trained epoch state.\")\n",
    "            final_metrics = evaluate_student(student_model, chhattisgarhi_val_loader, config, data_processor)\n",
    "\n",
    "\n",
    "    # Final logging\n",
    "    logger.info(\"====================== Final Results ======================\")\n",
    "    logger.info(f\"Best Student Validation Accuracy Achieved during training: {best_val_accuracy:.4f}\")\n",
    "    logger.info(\"Final Metrics (using best model if loaded, else last epoch):\")\n",
    "    if final_metrics:\n",
    "         logger.info(f\"  loss: {final_metrics.get('loss', float('nan')):.4f}\")\n",
    "         logger.info(f\"  perplexity: {final_metrics.get('perplexity', float('nan')):.4f}\")\n",
    "         logger.info(f\"  accuracy: {final_metrics.get('accuracy', float('nan')):.4f}\")\n",
    "         logger.info(f\"  precision: {final_metrics.get('precision', float('nan')):.4f}\")\n",
    "         logger.info(f\"  recall: {final_metrics.get('recall', float('nan')):.4f}\")\n",
    "         logger.info(f\"  f1: {final_metrics.get('f1', float('nan')):.4f}\")\n",
    "    else:\n",
    "         logger.warning(\"  Final metrics unavailable.\")\n",
    "    logger.info(\"===========================================================\")\n",
    "    logger.info(\"Training pipeline officially finished.\")\n",
    "\n",
    "\n",
    "    return teacher_model, student_model, metrics_history, final_metrics\n",
    "\n",
    "\n",
    "# --- Execution Block (Cell 49) ---\n",
    "# This block should ideally be separate from the main function definition\n",
    "# if __name__ == \"__main__\": # Use this if running as a script\n",
    "# Ensure logger is configured before this runs\n",
    "# if 'main' in globals(): # Check if main is defined before calling in notebook context\n",
    "#     logger.info(f\"Starting main execution block. CUDA_LAUNCH_BLOCKING is set to: {os.environ.get('CUDA_LAUNCH_BLOCKING', 'Not Set')}\")\n",
    "#     try:\n",
    "#         teacher_model, student_model, metrics_history, final_metrics = main()\n",
    "#         logger.info(\"Main execution completed successfully.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An error occurred during the main execution: {e}\", exc_info=True)\n",
    "# else:\n",
    "#     logger.error(\"The 'main' function is not defined. Please execute Cell 233 first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4838ac7-cae4-499e-b8c2-1d815ff4c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:34:14,203 - __main__ - INFO - CUDA_LAUNCH_BLOCKING is set to: 1\n",
      "2025-04-14 17:34:14,206 - __main__ - INFO - Configuration loaded. Using device: cuda\n",
      "2025-04-14 17:34:14,206 - __main__ - INFO - Teacher model: ai4bharat/indic-bert, Student model: distilbert-base-multilingual-cased\n",
      "2025-04-14 17:34:14,207 - __main__ - INFO - Max length: 64, Batch size: 32, Accumulation: 8\n",
      "2025-04-14 17:34:14,208 - __main__ - INFO - Initializing DataProcessor...\n",
      "2025-04-14 17:34:15,980 - __main__ - ERROR - Failed to initialize DataProcessor: 'DataProcessor' object has no attribute 'student_vocab_size'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9168\\548841237.py\", line 45, in main\n",
      "    logger.info(f\"Teacher vocab size: {data_processor.teacher_vocab_size}, Student vocab size: {data_processor.student_vocab_size}\")\n",
      "                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'DataProcessor' object has no attribute 'student_vocab_size'\n",
      "2025-04-14 17:34:15,981 - __main__ - ERROR - An error occurred during the main execution: 'DataProcessor' object has no attribute 'student_vocab_size'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9168\\1974365103.py\", line 6, in <module>\n",
      "    teacher_model, student_model, metrics_history, final_metrics = main()\n",
      "                                                                   ^^^^^^\n",
      "  File \"C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_9168\\548841237.py\", line 45, in main\n",
      "    logger.info(f\"Teacher vocab size: {data_processor.teacher_vocab_size}, Student vocab size: {data_processor.student_vocab_size}\")\n",
      "                                                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'DataProcessor' object has no attribute 'student_vocab_size'\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Execution Block (Add CUDA_LAUNCH_BLOCKING info)\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"CUDA_LAUNCH_BLOCKING is set to: {os.environ.get('CUDA_LAUNCH_BLOCKING', 'Not Set')}\")\n",
    "    try:\n",
    "        # ... (wandb init optional) ...\n",
    "        teacher_model, student_model, metrics_history, final_metrics = main()\n",
    "        # ... (Log final results) ...\n",
    "        logger.info(\"====================== Final Results ======================\")\n",
    "        if metrics_history:\n",
    "             best_acc = max(m['val_accuracy'] for m in metrics_history if not np.isnan(m['val_accuracy'])) if any(not np.isnan(m['val_accuracy']) for m in metrics_history) else float('nan')\n",
    "             logger.info(f\"Best Validation Accuracy Achieved: {best_acc:.4f}\")\n",
    "        logger.info(f\"Final Metrics (using best model):\")\n",
    "        for k, v in final_metrics.items():\n",
    "             logger.info(f\"  {k}: {v:.4f}\")\n",
    "        logger.info(\"===========================================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the main execution: {e}\", exc_info=True)\n",
    "        # ... (wandb finish optional) ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55022950-856f-429c-94c0-7619c787572c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b23763-c369-47a7-895c-495b32fabb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
